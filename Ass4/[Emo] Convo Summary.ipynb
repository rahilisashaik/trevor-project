{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convo Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we loook at different types of LLMS and how they summarize audio transcripts. Specifically we are interested in:\n",
    "- How concise each model summarizes the transcripts to\n",
    "- How understandable the summaries are- measured in terms of how quickly a receiver may view some measurement of \"urgency\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import openai\n",
    "from transformers import pipeline\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store a list of prompts to reuse for every model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "concise_prompt = \"Summarize the following audio transcript in 3–4 sentences. Focus on the main issue discussed, the speaker’s emotional state, and any signs of urgency or distress\"\n",
    "analytical_prompt = \"Read this conversation transcript and provide a concise summary that identifies the key concern, emotional tone, and any notable escalation or risk indicators. Use professional and neutral language.\"\n",
    "structured_prompt = \"Summarize the transcript with the following format:\\n- Main issue: \\n- Emotional state: \\n- Urgency level (low/medium/high):\"\n",
    "\n",
    "list_of_prompts = [concise_prompt, analytical_prompt, structured_prompt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by looking into summaries generated by ChatGPT 4o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the API Key, stored in seperate environment file\n",
    "load_dotenv()  # Loads from .env by default\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prompt ChatGPT\n",
    "def get_chatgpt_response(prompt):\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "def store_chatgpt_responses(list_of_prompts):\n",
    "    # Loop through each prompt\n",
    "    for prompt_num, prompt in enumerate(list_of_prompts, start=1):\n",
    "        # Define output path for this prompt's results\n",
    "        output_filename = f\"Summaries/gpt-4/prompt{prompt_num}_summary.txt\"\n",
    "        \n",
    "        # Create/open the file for writing all call summaries under this prompt\n",
    "        with open(output_filename, \"w\", encoding=\"utf-8\") as out_file:\n",
    "            # Loop through all transcripts (call01–call05)\n",
    "            for i in range(1, 6):\n",
    "                filename = f\"call{i:02d}.txt\"\n",
    "                filepath = f\"Transcripts/{filename}\"  # based on your folder structure\n",
    "\n",
    "                with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                    transcript_text = f.read()\n",
    "\n",
    "                # Combine prompt with transcript\n",
    "                full_prompt = f\"{prompt}\\n\\nTranscript:\\n{transcript_text}\"\n",
    "                response = get_chatgpt_response(full_prompt)\n",
    "\n",
    "                # Write formatted summary to the output file\n",
    "                out_file.write(f\"=== Summary for {filename} ===\\n\")\n",
    "                out_file.write(response.strip() + \"\\n\\n\")  # add space between summaries\n",
    "\n",
    "            print(f\"Saved all summaries for prompt {prompt_num} to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m store_chatgpt_responses(list_of_prompts)\n",
      "Cell \u001b[1;32mIn[21], line 33\u001b[0m, in \u001b[0;36mstore_chatgpt_responses\u001b[1;34m(list_of_prompts)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Combine prompt with transcript\u001b[39;00m\n\u001b[0;32m     32\u001b[0m full_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTranscript:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtranscript_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 33\u001b[0m response \u001b[38;5;241m=\u001b[39m get_chatgpt_response(full_prompt)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Write formatted summary to the output file\u001b[39;00m\n\u001b[0;32m     36\u001b[0m out_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Summary for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ===\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[21], line 5\u001b[0m, in \u001b[0;36mget_chatgpt_response\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_chatgpt_response\u001b[39m(prompt):\n\u001b[1;32m----> 5\u001b[0m     completion \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m      6\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      8\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m      9\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}\n\u001b[0;32m     10\u001b[0m         ]\n\u001b[0;32m     11\u001b[0m     )\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32mc:\\Users\\emily\\anaconda3\\Lib\\site-packages\\openai\\_utils\\_proxy.py:20\u001b[0m, in \u001b[0;36mLazyProxy.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m---> 20\u001b[0m     proxied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_proxied__()\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(proxied, LazyProxy):\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m proxied  \u001b[38;5;66;03m# pyright: ignore\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\emily\\anaconda3\\Lib\\site-packages\\openai\\_utils\\_proxy.py:55\u001b[0m, in \u001b[0;36mLazyProxy.__get_proxied__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__get_proxied__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__load__()\n",
      "File \u001b[1;32mc:\\Users\\emily\\anaconda3\\Lib\\site-packages\\openai\\_module_client.py:12\u001b[0m, in \u001b[0;36mChatProxy.__load__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__load__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m resources\u001b[38;5;241m.\u001b[39mChat:\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_client()\u001b[38;5;241m.\u001b[39mchat\n",
      "File \u001b[1;32mc:\\Users\\emily\\anaconda3\\Lib\\site-packages\\openai\\__init__.py:329\u001b[0m, in \u001b[0;36m_load_client\u001b[1;34m()\u001b[0m\n\u001b[0;32m    313\u001b[0m         _client \u001b[38;5;241m=\u001b[39m _AzureModuleClient(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    314\u001b[0m             api_version\u001b[38;5;241m=\u001b[39mapi_version,\n\u001b[0;32m    315\u001b[0m             azure_endpoint\u001b[38;5;241m=\u001b[39mazure_endpoint,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    325\u001b[0m             http_client\u001b[38;5;241m=\u001b[39mhttp_client,\n\u001b[0;32m    326\u001b[0m         )\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _client\n\u001b[1;32m--> 329\u001b[0m     _client \u001b[38;5;241m=\u001b[39m _ModuleClient(\n\u001b[0;32m    330\u001b[0m         api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[0;32m    331\u001b[0m         organization\u001b[38;5;241m=\u001b[39morganization,\n\u001b[0;32m    332\u001b[0m         project\u001b[38;5;241m=\u001b[39mproject,\n\u001b[0;32m    333\u001b[0m         base_url\u001b[38;5;241m=\u001b[39mbase_url,\n\u001b[0;32m    334\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    335\u001b[0m         max_retries\u001b[38;5;241m=\u001b[39mmax_retries,\n\u001b[0;32m    336\u001b[0m         default_headers\u001b[38;5;241m=\u001b[39mdefault_headers,\n\u001b[0;32m    337\u001b[0m         default_query\u001b[38;5;241m=\u001b[39mdefault_query,\n\u001b[0;32m    338\u001b[0m         http_client\u001b[38;5;241m=\u001b[39mhttp_client,\n\u001b[0;32m    339\u001b[0m     )\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _client\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _client\n",
      "File \u001b[1;32mc:\\Users\\emily\\anaconda3\\Lib\\site-packages\\openai\\_client.py:114\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[1;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[0;32m    112\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    116\u001b[0m     )\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "store_chatgpt_responses(list_of_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each txt file, we are able to see how each prompt summarizes the five different audio transcripts. We observe that the **structured_prompt** does the best in summarizing in **the most organized/easily understandable manner**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also wanted to try summarizing with BART, a model pre-trained on English language and fine-tuned on CNN Daily Mail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "727de28f199c4889bce63518dc9ded26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emily\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\emily\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20015d037b1d40a7aef69d4617bd038e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd016b0c0d3412a8fa3a882ad2ac2ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86d256af631e4fbb8980ebaa1cd9ebe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91574aea444f4c869c015ca85209d1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Load BART summarization pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Function to generate summary using BART\n",
    "def get_bart_summary(text, max_input_length=1024, max_output_length=200):\n",
    "    # Truncate input if it exceeds BART's limit (1024 tokens)\n",
    "    truncated_text = text[:max_input_length]\n",
    "\n",
    "    summary = summarizer(\n",
    "        truncated_text,\n",
    "        max_length=max_output_length,\n",
    "        min_length=30,\n",
    "        do_sample=False\n",
    "    )\n",
    "    return summary[0]['summary_text']\n",
    "\n",
    "\n",
    "# Function to store summaries using BART\n",
    "def store_bart_summaries(list_of_prompts):\n",
    "    os.makedirs(\"Summaries/bart\", exist_ok=True)\n",
    "\n",
    "    for prompt_num, prompt in enumerate(list_of_prompts, start=1):\n",
    "        output_filename = f\"Summaries/bart/prompt{prompt_num}_summary.txt\"\n",
    "\n",
    "        with open(output_filename, \"w\", encoding=\"utf-8\") as out_file:\n",
    "            for i in range(1, 6):\n",
    "                filename = f\"call{i:02d}.txt\"\n",
    "                filepath = f\"Transcripts/{filename}\"\n",
    "\n",
    "                with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                    transcript_text = f.read()\n",
    "\n",
    "                # Combine prompt with transcript\n",
    "                full_input = f\"{prompt}\\n\\nTranscript:\\n{transcript_text}\"\n",
    "\n",
    "                # Generate summary using BART\n",
    "                summary = get_bart_summary(full_input)\n",
    "\n",
    "                # Write the summary to the output file\n",
    "                out_file.write(f\"=== Summary for {filename} ===\\n\")\n",
    "                out_file.write(summary.strip() + \"\\n\\n\")\n",
    "\n",
    "            print(f\"Saved all summaries for prompt {prompt_num} to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all summaries for prompt 1 to Summaries/bart/prompt1_summary.txt\n",
      "Saved all summaries for prompt 2 to Summaries/bart/prompt2_summary.txt\n",
      "Saved all summaries for prompt 3 to Summaries/bart/prompt3_summary.txt\n"
     ]
    }
   ],
   "source": [
    "store_bart_summaries(list_of_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the summaries produced by bart, it's clear that the model attempts to summarize more like a new article summary than an audio transcript summarization. Bart is not a suitable model for summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
